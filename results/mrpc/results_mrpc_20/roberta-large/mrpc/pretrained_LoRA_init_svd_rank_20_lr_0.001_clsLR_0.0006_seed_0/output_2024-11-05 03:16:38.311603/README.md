---
language:
- en
license: mit
library_name: peft
tags:
- generated_from_trainer
datasets:
- glue
metrics:
- accuracy
- f1
base_model: roberta-large
model-index:
- name: output_2024-11-05 03:16:38.311603
  results:
  - task:
      type: text-classification
      name: Text Classification
    dataset:
      name: GLUE MRPC
      type: glue
      args: mrpc
    metrics:
    - type: accuracy
      value: 0.8823529411764706
      name: Accuracy
    - type: f1
      value: 0.9148936170212766
      name: F1
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# output_2024-11-05 03:16:38.311603

This model is a fine-tuned version of [roberta-large](https://huggingface.co/roberta-large) on the GLUE MRPC dataset.
It achieves the following results on the evaluation set:
- Loss: 0.8551
- Accuracy: 0.8824
- F1: 0.9149
- Combined Score: 0.8986

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 0.001
- train_batch_size: 12
- eval_batch_size: 8
- seed: 0
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: linear
- num_epochs: 50.0

### Training results

| Training Loss | Epoch | Step  | Validation Loss | Accuracy | F1     | Combined Score |
|:-------------:|:-----:|:-----:|:---------------:|:--------:|:------:|:--------------:|
| 0.4243        | 1.0   | 306   | 0.4363          | 0.8260   | 0.8838 | 0.8549         |
| 0.252         | 2.0   | 612   | 0.3314          | 0.8603   | 0.8984 | 0.8793         |
| 0.3019        | 3.0   | 918   | 0.2994          | 0.8824   | 0.9121 | 0.8972         |
| 0.2437        | 4.0   | 1224  | 0.3311          | 0.8775   | 0.9120 | 0.8947         |
| 0.2629        | 5.0   | 1530  | 0.3001          | 0.8922   | 0.9228 | 0.9075         |
| 0.2606        | 6.0   | 1836  | 0.2780          | 0.8995   | 0.9282 | 0.9139         |
| 0.2181        | 7.0   | 2142  | 0.3008          | 0.8995   | 0.9289 | 0.9142         |
| 0.1798        | 8.0   | 2448  | 0.3581          | 0.8995   | 0.9287 | 0.9141         |
| 0.2211        | 9.0   | 2754  | 0.3562          | 0.8922   | 0.9211 | 0.9067         |
| 0.2247        | 10.0  | 3060  | 0.3477          | 0.8995   | 0.9256 | 0.9125         |
| 0.282         | 11.0  | 3366  | 0.3044          | 0.8946   | 0.9242 | 0.9094         |
| 0.2307        | 12.0  | 3672  | 0.4951          | 0.8701   | 0.9103 | 0.8902         |
| 0.183         | 13.0  | 3978  | 0.4228          | 0.8897   | 0.9220 | 0.9059         |
| 0.0696        | 14.0  | 4284  | 0.4138          | 0.9044   | 0.9312 | 0.9178         |
| 0.1633        | 15.0  | 4590  | 0.4253          | 0.8873   | 0.9184 | 0.9028         |
| 0.2482        | 16.0  | 4896  | 0.3395          | 0.8995   | 0.9279 | 0.9137         |
| 0.14          | 17.0  | 5202  | 0.4440          | 0.9020   | 0.9301 | 0.9160         |
| 0.1327        | 18.0  | 5508  | 0.4557          | 0.8922   | 0.9231 | 0.9076         |
| 0.0849        | 19.0  | 5814  | 0.5017          | 0.8897   | 0.9206 | 0.9052         |
| 0.172         | 20.0  | 6120  | 0.4504          | 0.8971   | 0.9261 | 0.9116         |
| 0.1254        | 21.0  | 6426  | 0.5247          | 0.8848   | 0.9180 | 0.9014         |
| 0.1045        | 22.0  | 6732  | 0.5595          | 0.8873   | 0.9201 | 0.9037         |
| 0.0814        | 23.0  | 7038  | 0.5762          | 0.8873   | 0.9199 | 0.9036         |
| 0.0626        | 24.0  | 7344  | 0.4932          | 0.8971   | 0.9242 | 0.9106         |
| 0.0622        | 25.0  | 7650  | 0.6412          | 0.8897   | 0.9204 | 0.9050         |
| 0.1041        | 26.0  | 7956  | 0.5436          | 0.8922   | 0.9220 | 0.9071         |
| 0.1017        | 27.0  | 8262  | 0.7348          | 0.8824   | 0.9170 | 0.8997         |
| 0.1696        | 28.0  | 8568  | 0.5339          | 0.8824   | 0.9143 | 0.8983         |
| 0.1654        | 29.0  | 8874  | 0.6781          | 0.8775   | 0.9132 | 0.8953         |
| 0.0384        | 30.0  | 9180  | 0.7186          | 0.8848   | 0.9180 | 0.9014         |
| 0.048         | 31.0  | 9486  | 0.7121          | 0.875    | 0.9116 | 0.8933         |
| 0.1021        | 32.0  | 9792  | 0.8170          | 0.8775   | 0.9132 | 0.8953         |
| 0.0817        | 33.0  | 10098 | 0.6744          | 0.8848   | 0.9171 | 0.9010         |
| 0.0621        | 34.0  | 10404 | 0.6933          | 0.8725   | 0.9100 | 0.8913         |
| 0.0389        | 35.0  | 10710 | 0.7594          | 0.8897   | 0.9217 | 0.9057         |
| 0.0355        | 36.0  | 11016 | 0.8272          | 0.8897   | 0.9206 | 0.9052         |
| 0.0689        | 37.0  | 11322 | 0.7423          | 0.8725   | 0.9094 | 0.8910         |
| 0.0348        | 38.0  | 11628 | 0.8738          | 0.8799   | 0.9148 | 0.8973         |
| 0.0593        | 39.0  | 11934 | 0.8192          | 0.8775   | 0.9135 | 0.8955         |
| 0.0402        | 40.0  | 12240 | 0.8505          | 0.8824   | 0.9152 | 0.8988         |
| 0.0184        | 41.0  | 12546 | 0.8672          | 0.8922   | 0.9236 | 0.9079         |
| 0.1238        | 42.0  | 12852 | 0.7698          | 0.8897   | 0.9206 | 0.9052         |
| 0.0537        | 43.0  | 13158 | 0.7996          | 0.8848   | 0.9171 | 0.9010         |
| 0.0361        | 44.0  | 13464 | 0.7678          | 0.8897   | 0.9204 | 0.9050         |
| 0.0182        | 45.0  | 13770 | 0.8067          | 0.8848   | 0.9162 | 0.9005         |
| 0.0765        | 46.0  | 14076 | 0.8167          | 0.8799   | 0.9133 | 0.8966         |
| 0.0341        | 47.0  | 14382 | 0.8467          | 0.8725   | 0.9085 | 0.8905         |
| 0.0126        | 48.0  | 14688 | 0.8771          | 0.8799   | 0.9136 | 0.8967         |
| 0.0475        | 49.0  | 14994 | 0.8550          | 0.8775   | 0.9113 | 0.8944         |
| 0.041         | 50.0  | 15300 | 0.8551          | 0.8824   | 0.9149 | 0.8986         |


### Framework versions

- PEFT 0.10.0
- Transformers 4.40.1
- Pytorch 2.2.1+cu121
- Datasets 2.16.1
- Tokenizers 0.19.1