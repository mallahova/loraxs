---
language:
- en
license: mit
library_name: peft
tags:
- generated_from_trainer
datasets:
- glue
metrics:
- accuracy
- f1
base_model: roberta-large
model-index:
- name: output_2024-11-03 18:58:48.618214
  results:
  - task:
      type: text-classification
      name: Text Classification
    dataset:
      name: GLUE MRPC
      type: glue
      args: mrpc
    metrics:
    - type: accuracy
      value: 0.8602941176470589
      name: Accuracy
    - type: f1
      value: 0.900523560209424
      name: F1
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# output_2024-11-03 18:58:48.618214

This model is a fine-tuned version of [roberta-large](https://huggingface.co/roberta-large) on the GLUE MRPC dataset.
It achieves the following results on the evaluation set:
- Loss: 0.5753
- Accuracy: 0.8603
- F1: 0.9005
- Combined Score: 0.8804

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 0.0006
- train_batch_size: 8
- eval_batch_size: 8
- seed: 0
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: linear
- num_epochs: 50.0

### Training results

| Training Loss | Epoch | Step  | Validation Loss | Accuracy | F1     | Combined Score |
|:-------------:|:-----:|:-----:|:---------------:|:--------:|:------:|:--------------:|
| 0.5654        | 1.0   | 459   | 0.4994          | 0.8039   | 0.8671 | 0.8355         |
| 0.5494        | 2.0   | 918   | 0.4242          | 0.8235   | 0.875  | 0.8493         |
| 0.4717        | 3.0   | 1377  | 0.3739          | 0.8554   | 0.8974 | 0.8764         |
| 0.4002        | 4.0   | 1836  | 0.4189          | 0.8162   | 0.8784 | 0.8473         |
| 0.317         | 5.0   | 2295  | 0.6023          | 0.8186   | 0.8803 | 0.8494         |
| 0.4421        | 6.0   | 2754  | 0.3575          | 0.8480   | 0.8912 | 0.8696         |
| 0.4806        | 7.0   | 3213  | 0.4437          | 0.8554   | 0.9002 | 0.8778         |
| 0.4205        | 8.0   | 3672  | 0.3823          | 0.8603   | 0.8995 | 0.8799         |
| 0.2904        | 9.0   | 4131  | 0.3976          | 0.8701   | 0.9042 | 0.8871         |
| 0.4027        | 10.0  | 4590  | 0.3450          | 0.8701   | 0.9065 | 0.8883         |
| 0.442         | 11.0  | 5049  | 0.3728          | 0.8554   | 0.8998 | 0.8776         |
| 0.386         | 12.0  | 5508  | 0.3873          | 0.8725   | 0.9100 | 0.8913         |
| 0.3104        | 13.0  | 5967  | 0.4412          | 0.8578   | 0.9033 | 0.8806         |
| 0.2436        | 14.0  | 6426  | 0.4186          | 0.8554   | 0.8937 | 0.8745         |
| 0.5282        | 15.0  | 6885  | 0.3757          | 0.8627   | 0.9031 | 0.8829         |
| 0.3542        | 16.0  | 7344  | 0.3273          | 0.8824   | 0.9172 | 0.8998         |
| 0.3828        | 17.0  | 7803  | 0.4530          | 0.8676   | 0.9082 | 0.8879         |
| 0.3331        | 18.0  | 8262  | 0.3476          | 0.8775   | 0.9120 | 0.8947         |
| 0.2552        | 19.0  | 8721  | 0.5803          | 0.8333   | 0.8859 | 0.8596         |
| 0.3826        | 20.0  | 9180  | 0.4573          | 0.8505   | 0.8975 | 0.8740         |
| 0.2951        | 21.0  | 9639  | 0.3771          | 0.8676   | 0.9036 | 0.8856         |
| 0.2724        | 22.0  | 10098 | 0.4009          | 0.8725   | 0.9071 | 0.8898         |
| 0.3018        | 23.0  | 10557 | 0.4188          | 0.8578   | 0.8986 | 0.8782         |
| 0.252         | 24.0  | 11016 | 0.4336          | 0.8407   | 0.8841 | 0.8624         |
| 0.331         | 25.0  | 11475 | 0.4721          | 0.8554   | 0.8988 | 0.8771         |
| 0.2805        | 26.0  | 11934 | 0.4438          | 0.8578   | 0.9014 | 0.8796         |
| 0.3053        | 27.0  | 12393 | 0.5238          | 0.8554   | 0.9002 | 0.8778         |
| 0.2601        | 28.0  | 12852 | 0.4654          | 0.8456   | 0.8877 | 0.8666         |
| 0.3617        | 29.0  | 13311 | 0.5435          | 0.8382   | 0.8838 | 0.8610         |
| 0.3016        | 30.0  | 13770 | 0.4752          | 0.8505   | 0.8924 | 0.8715         |
| 0.2787        | 31.0  | 14229 | 0.4475          | 0.8529   | 0.8932 | 0.8731         |
| 0.2023        | 32.0  | 14688 | 0.5640          | 0.8456   | 0.8912 | 0.8684         |
| 0.2248        | 33.0  | 15147 | 0.5201          | 0.8456   | 0.8893 | 0.8674         |
| 0.1358        | 34.0  | 15606 | 0.5812          | 0.8505   | 0.8935 | 0.8720         |
| 0.3085        | 35.0  | 16065 | 0.5092          | 0.8480   | 0.8905 | 0.8692         |
| 0.2552        | 36.0  | 16524 | 0.5320          | 0.8529   | 0.8932 | 0.8731         |
| 0.2582        | 37.0  | 16983 | 0.5423          | 0.8603   | 0.8988 | 0.8795         |
| 0.2204        | 38.0  | 17442 | 0.5324          | 0.8578   | 0.8982 | 0.8780         |
| 0.0951        | 39.0  | 17901 | 0.5324          | 0.8578   | 0.8986 | 0.8782         |
| 0.155         | 40.0  | 18360 | 0.5921          | 0.8578   | 0.8975 | 0.8777         |
| 0.1066        | 41.0  | 18819 | 0.5698          | 0.8480   | 0.8908 | 0.8694         |
| 0.132         | 42.0  | 19278 | 0.4966          | 0.8578   | 0.8979 | 0.8779         |
| 0.0966        | 43.0  | 19737 | 0.5340          | 0.8676   | 0.9053 | 0.8865         |
| 0.1957        | 44.0  | 20196 | 0.5628          | 0.8529   | 0.8921 | 0.8725         |
| 0.189         | 45.0  | 20655 | 0.5415          | 0.8676   | 0.9039 | 0.8858         |
| 0.1941        | 46.0  | 21114 | 0.6108          | 0.8603   | 0.9012 | 0.8808         |
| 0.1282        | 47.0  | 21573 | 0.5663          | 0.8627   | 0.9018 | 0.8822         |
| 0.1085        | 48.0  | 22032 | 0.5760          | 0.8554   | 0.8970 | 0.8762         |
| 0.2328        | 49.0  | 22491 | 0.5691          | 0.8627   | 0.9018 | 0.8822         |
| 0.1971        | 50.0  | 22950 | 0.5753          | 0.8603   | 0.9005 | 0.8804         |


### Framework versions

- PEFT 0.10.0
- Transformers 4.40.1
- Pytorch 2.2.1+cu121
- Datasets 2.16.1
- Tokenizers 0.19.1