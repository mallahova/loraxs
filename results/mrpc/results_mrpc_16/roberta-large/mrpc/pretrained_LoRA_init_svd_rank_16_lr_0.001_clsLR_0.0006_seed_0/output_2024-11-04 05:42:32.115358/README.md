---
language:
- en
license: mit
library_name: peft
tags:
- generated_from_trainer
datasets:
- glue
metrics:
- accuracy
- f1
base_model: roberta-large
model-index:
- name: output_2024-11-04 05:42:32.115358
  results:
  - task:
      type: text-classification
      name: Text Classification
    dataset:
      name: GLUE MRPC
      type: glue
      args: mrpc
    metrics:
    - type: accuracy
      value: 0.8921568627450981
      name: Accuracy
    - type: f1
      value: 0.9222614840989399
      name: F1
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# output_2024-11-04 05:42:32.115358

This model is a fine-tuned version of [roberta-large](https://huggingface.co/roberta-large) on the GLUE MRPC dataset.
It achieves the following results on the evaluation set:
- Loss: 0.7351
- Accuracy: 0.8922
- F1: 0.9223
- Combined Score: 0.9072

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 0.001
- train_batch_size: 8
- eval_batch_size: 8
- seed: 0
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: linear
- num_epochs: 50.0

### Training results

| Training Loss | Epoch | Step  | Validation Loss | Accuracy | F1     | Combined Score |
|:-------------:|:-----:|:-----:|:---------------:|:--------:|:------:|:--------------:|
| 0.4956        | 1.0   | 459   | 0.4489          | 0.8676   | 0.9088 | 0.8882         |
| 0.5271        | 2.0   | 918   | 0.3374          | 0.8676   | 0.9007 | 0.8842         |
| 0.384         | 3.0   | 1377  | 0.2801          | 0.8897   | 0.9186 | 0.9042         |
| 0.3078        | 4.0   | 1836  | 0.3914          | 0.8652   | 0.9066 | 0.8859         |
| 0.2142        | 5.0   | 2295  | 0.3330          | 0.8922   | 0.9225 | 0.9073         |
| 0.2335        | 6.0   | 2754  | 0.3417          | 0.8725   | 0.9078 | 0.8902         |
| 0.2746        | 7.0   | 3213  | 0.4421          | 0.8799   | 0.9154 | 0.8976         |
| 0.2728        | 8.0   | 3672  | 0.4016          | 0.8873   | 0.9167 | 0.9020         |
| 0.1322        | 9.0   | 4131  | 0.4255          | 0.8922   | 0.9214 | 0.9068         |
| 0.229         | 10.0  | 4590  | 0.3673          | 0.8995   | 0.9264 | 0.9130         |
| 0.3106        | 11.0  | 5049  | 0.4008          | 0.875    | 0.9104 | 0.8927         |
| 0.1858        | 12.0  | 5508  | 0.5301          | 0.8848   | 0.9177 | 0.9012         |
| 0.2468        | 13.0  | 5967  | 0.3961          | 0.8971   | 0.9247 | 0.9109         |
| 0.1334        | 14.0  | 6426  | 0.4831          | 0.8922   | 0.9211 | 0.9067         |
| 0.201         | 15.0  | 6885  | 0.4784          | 0.8873   | 0.9176 | 0.9024         |
| 0.1512        | 16.0  | 7344  | 0.4513          | 0.8848   | 0.9162 | 0.9005         |
| 0.2383        | 17.0  | 7803  | 0.4864          | 0.8799   | 0.9154 | 0.8976         |
| 0.1428        | 18.0  | 8262  | 0.5265          | 0.8995   | 0.9242 | 0.9119         |
| 0.2085        | 19.0  | 8721  | 0.4932          | 0.8873   | 0.9196 | 0.9034         |
| 0.208         | 20.0  | 9180  | 0.4614          | 0.8848   | 0.9171 | 0.9010         |
| 0.3122        | 21.0  | 9639  | 0.5171          | 0.8897   | 0.9209 | 0.9053         |
| 0.0132        | 22.0  | 10098 | 0.6357          | 0.8873   | 0.9179 | 0.9026         |
| 0.2138        | 23.0  | 10557 | 0.5054          | 0.8995   | 0.9245 | 0.9120         |
| 0.0364        | 24.0  | 11016 | 0.6155          | 0.8824   | 0.9149 | 0.8986         |
| 0.0663        | 25.0  | 11475 | 0.6745          | 0.8775   | 0.9129 | 0.8952         |
| 0.2405        | 26.0  | 11934 | 0.5795          | 0.875    | 0.9097 | 0.8924         |
| 0.1862        | 27.0  | 12393 | 0.7125          | 0.8775   | 0.9117 | 0.8946         |
| 0.1854        | 28.0  | 12852 | 0.6397          | 0.8701   | 0.9069 | 0.8885         |
| 0.1136        | 29.0  | 13311 | 0.7440          | 0.8922   | 0.9228 | 0.9075         |
| 0.062         | 30.0  | 13770 | 0.6958          | 0.8971   | 0.9247 | 0.9109         |
| 0.146         | 31.0  | 14229 | 0.5997          | 0.8873   | 0.9181 | 0.9027         |
| 0.1824        | 32.0  | 14688 | 0.6212          | 0.8848   | 0.9168 | 0.9008         |
| 0.0888        | 33.0  | 15147 | 0.6273          | 0.8873   | 0.9187 | 0.9030         |
| 0.0371        | 34.0  | 15606 | 0.7165          | 0.8873   | 0.9190 | 0.9031         |
| 0.124         | 35.0  | 16065 | 0.6380          | 0.8897   | 0.9192 | 0.9045         |
| 0.0416        | 36.0  | 16524 | 0.7937          | 0.8824   | 0.9158 | 0.8991         |
| 0.0667        | 37.0  | 16983 | 0.6611          | 0.8971   | 0.9258 | 0.9114         |
| 0.0528        | 38.0  | 17442 | 0.8254          | 0.8897   | 0.9206 | 0.9052         |
| 0.0013        | 39.0  | 17901 | 0.7177          | 0.8946   | 0.9236 | 0.9091         |
| 0.0045        | 40.0  | 18360 | 0.7388          | 0.8995   | 0.9261 | 0.9128         |
| 0.069         | 41.0  | 18819 | 0.7760          | 0.8873   | 0.9193 | 0.9033         |
| 0.0417        | 42.0  | 19278 | 0.7207          | 0.8922   | 0.9220 | 0.9071         |
| 0.0179        | 43.0  | 19737 | 0.7595          | 0.8946   | 0.9231 | 0.9088         |
| 0.108         | 44.0  | 20196 | 0.7850          | 0.8971   | 0.9236 | 0.9103         |
| 0.0056        | 45.0  | 20655 | 0.7452          | 0.8995   | 0.9269 | 0.9132         |
| 0.0812        | 46.0  | 21114 | 0.7445          | 0.8873   | 0.9173 | 0.9023         |
| 0.0007        | 47.0  | 21573 | 0.7201          | 0.9020   | 0.9283 | 0.9151         |
| 0.022         | 48.0  | 22032 | 0.7791          | 0.8799   | 0.9142 | 0.8970         |
| 0.0342        | 49.0  | 22491 | 0.7473          | 0.8897   | 0.9206 | 0.9052         |
| 0.0167        | 50.0  | 22950 | 0.7351          | 0.8922   | 0.9223 | 0.9072         |


### Framework versions

- PEFT 0.10.0
- Transformers 4.40.1
- Pytorch 2.2.1+cu121
- Datasets 2.16.1
- Tokenizers 0.19.1