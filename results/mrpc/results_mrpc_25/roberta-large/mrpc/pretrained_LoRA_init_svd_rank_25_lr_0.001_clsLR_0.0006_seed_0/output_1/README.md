---
language:
- en
license: mit
library_name: peft
tags:
- generated_from_trainer
datasets:
- glue
metrics:
- accuracy
- f1
base_model: roberta-large
model-index:
- name: output_1
  results:
  - task:
      type: text-classification
      name: Text Classification
    dataset:
      name: GLUE MRPC
      type: glue
      args: mrpc
    metrics:
    - type: accuracy
      value: 0.8970588235294118
      name: Accuracy
    - type: f1
      value: 0.926056338028169
      name: F1
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# output_1

This model is a fine-tuned version of [roberta-large](https://huggingface.co/roberta-large) on the GLUE MRPC dataset.
It achieves the following results on the evaluation set:
- Loss: 0.8833
- Accuracy: 0.8971
- F1: 0.9261
- Combined Score: 0.9116

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 0.001
- train_batch_size: 12
- eval_batch_size: 8
- seed: 0
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: linear
- num_epochs: 70.0

### Training results

| Training Loss | Epoch | Step  | Validation Loss | Accuracy | F1     | Combined Score |
|:-------------:|:-----:|:-----:|:---------------:|:--------:|:------:|:--------------:|
| 0.4096        | 1.0   | 306   | 0.4233          | 0.8309   | 0.8856 | 0.8582         |
| 0.285         | 2.0   | 612   | 0.3243          | 0.8701   | 0.9020 | 0.8861         |
| 0.2897        | 3.0   | 918   | 0.3628          | 0.8922   | 0.92   | 0.9061         |
| 0.2099        | 4.0   | 1224  | 0.3875          | 0.8922   | 0.9220 | 0.9071         |
| 0.2226        | 5.0   | 1530  | 0.3378          | 0.8995   | 0.9274 | 0.9135         |
| 0.2417        | 6.0   | 1836  | 0.3283          | 0.8995   | 0.9279 | 0.9137         |
| 0.1577        | 7.0   | 2142  | 0.4572          | 0.8824   | 0.9181 | 0.9002         |
| 0.2108        | 8.0   | 2448  | 0.3761          | 0.8897   | 0.9223 | 0.9060         |
| 0.2206        | 9.0   | 2754  | 0.4206          | 0.8873   | 0.9190 | 0.9031         |
| 0.1006        | 10.0  | 3060  | 0.4048          | 0.8971   | 0.9253 | 0.9112         |
| 0.1504        | 11.0  | 3366  | 0.4266          | 0.8995   | 0.9269 | 0.9132         |
| 0.1906        | 12.0  | 3672  | 0.4829          | 0.8995   | 0.9264 | 0.9130         |
| 0.1172        | 13.0  | 3978  | 0.4222          | 0.8873   | 0.9201 | 0.9037         |
| 0.0952        | 14.0  | 4284  | 0.4989          | 0.8946   | 0.9242 | 0.9094         |
| 0.1482        | 15.0  | 4590  | 0.5619          | 0.8824   | 0.9134 | 0.8979         |
| 0.2109        | 16.0  | 4896  | 0.5236          | 0.8799   | 0.9130 | 0.8964         |
| 0.1484        | 17.0  | 5202  | 0.6279          | 0.8799   | 0.9148 | 0.8973         |
| 0.0776        | 18.0  | 5508  | 0.5836          | 0.9020   | 0.9291 | 0.9155         |
| 0.0897        | 19.0  | 5814  | 0.5740          | 0.9020   | 0.9293 | 0.9156         |
| 0.0974        | 20.0  | 6120  | 0.5807          | 0.8873   | 0.9201 | 0.9037         |
| 0.0912        | 21.0  | 6426  | 0.8281          | 0.8652   | 0.9069 | 0.8861         |
| 0.0452        | 22.0  | 6732  | 0.6501          | 0.8897   | 0.9217 | 0.9057         |
| 0.0859        | 23.0  | 7038  | 0.6367          | 0.8897   | 0.9217 | 0.9057         |
| 0.0681        | 24.0  | 7344  | 0.5604          | 0.9020   | 0.9281 | 0.9150         |
| 0.0287        | 25.0  | 7650  | 0.5634          | 0.9020   | 0.9291 | 0.9155         |
| 0.1012        | 26.0  | 7956  | 0.6148          | 0.8946   | 0.9231 | 0.9088         |
| 0.0982        | 27.0  | 8262  | 0.6739          | 0.9020   | 0.9286 | 0.9153         |
| 0.1175        | 28.0  | 8568  | 0.5161          | 0.8971   | 0.9250 | 0.9110         |
| 0.053         | 29.0  | 8874  | 0.6993          | 0.875    | 0.9107 | 0.8928         |
| 0.0567        | 30.0  | 9180  | 0.6173          | 0.8824   | 0.9167 | 0.8995         |
| 0.0522        | 31.0  | 9486  | 0.5888          | 0.8995   | 0.9269 | 0.9132         |
| 0.0308        | 32.0  | 9792  | 0.7587          | 0.8873   | 0.9196 | 0.9034         |
| 0.1025        | 33.0  | 10098 | 0.6976          | 0.8848   | 0.9188 | 0.9018         |
| 0.0632        | 34.0  | 10404 | 0.6210          | 0.8946   | 0.9239 | 0.9093         |
| 0.0466        | 35.0  | 10710 | 0.6931          | 0.9044   | 0.9307 | 0.9176         |
| 0.0372        | 36.0  | 11016 | 0.7389          | 0.9044   | 0.9295 | 0.9169         |
| 0.0788        | 37.0  | 11322 | 0.5792          | 0.9069   | 0.9324 | 0.9196         |
| 0.1542        | 38.0  | 11628 | 0.6912          | 0.8971   | 0.9255 | 0.9113         |
| 0.0689        | 39.0  | 11934 | 0.6293          | 0.8946   | 0.9236 | 0.9091         |
| 0.0183        | 40.0  | 12240 | 0.8008          | 0.8971   | 0.9266 | 0.9118         |
| 0.0016        | 41.0  | 12546 | 0.8030          | 0.8897   | 0.9217 | 0.9057         |
| 0.0307        | 42.0  | 12852 | 0.8475          | 0.8873   | 0.9199 | 0.9036         |
| 0.0315        | 43.0  | 13158 | 0.8336          | 0.8971   | 0.9253 | 0.9112         |
| 0.0428        | 44.0  | 13464 | 0.7150          | 0.8995   | 0.9272 | 0.9133         |
| 0.001         | 45.0  | 13770 | 0.8404          | 0.9020   | 0.9291 | 0.9155         |
| 0.0207        | 46.0  | 14076 | 0.7868          | 0.8971   | 0.9266 | 0.9118         |
| 0.0035        | 47.0  | 14382 | 0.7807          | 0.9044   | 0.9310 | 0.9177         |
| 0.0011        | 48.0  | 14688 | 0.9113          | 0.9069   | 0.9321 | 0.9195         |
| 0.013         | 49.0  | 14994 | 0.8695          | 0.8971   | 0.9261 | 0.9116         |
| 0.0236        | 50.0  | 15300 | 0.8911          | 0.9020   | 0.9281 | 0.9150         |
| 0.0736        | 51.0  | 15606 | 0.8616          | 0.8922   | 0.9220 | 0.9071         |
| 0.0037        | 52.0  | 15912 | 0.8151          | 0.8922   | 0.9228 | 0.9075         |
| 0.001         | 53.0  | 16218 | 0.7987          | 0.8922   | 0.9233 | 0.9078         |
| 0.0214        | 54.0  | 16524 | 0.8566          | 0.8848   | 0.9159 | 0.9004         |
| 0.0089        | 55.0  | 16830 | 0.7733          | 0.8995   | 0.9267 | 0.9131         |
| 0.0188        | 56.0  | 17136 | 0.7736          | 0.9044   | 0.9305 | 0.9174         |
| 0.0217        | 57.0  | 17442 | 0.7946          | 0.8922   | 0.9223 | 0.9072         |
| 0.0061        | 58.0  | 17748 | 0.8207          | 0.8922   | 0.9217 | 0.9069         |
| 0.0238        | 59.0  | 18054 | 0.8521          | 0.8897   | 0.9204 | 0.9050         |
| 0.0002        | 60.0  | 18360 | 0.8368          | 0.8897   | 0.9204 | 0.9050         |
| 0.0002        | 61.0  | 18666 | 0.8636          | 0.8946   | 0.9236 | 0.9091         |
| 0.042         | 62.0  | 18972 | 0.8543          | 0.8922   | 0.9223 | 0.9072         |
| 0.0018        | 63.0  | 19278 | 0.8445          | 0.8922   | 0.9223 | 0.9072         |
| 0.0058        | 64.0  | 19584 | 0.8709          | 0.8946   | 0.9244 | 0.9095         |
| 0.0003        | 65.0  | 19890 | 0.8459          | 0.8971   | 0.9261 | 0.9116         |
| 0.0038        | 66.0  | 20196 | 0.9022          | 0.8922   | 0.9228 | 0.9075         |
| 0.0029        | 67.0  | 20502 | 0.8624          | 0.8995   | 0.9277 | 0.9136         |
| 0.0008        | 68.0  | 20808 | 0.8689          | 0.8995   | 0.9277 | 0.9136         |
| 0.0058        | 69.0  | 21114 | 0.8869          | 0.8946   | 0.9244 | 0.9095         |
| 0.0269        | 70.0  | 21420 | 0.8833          | 0.8971   | 0.9261 | 0.9116         |


### Framework versions

- PEFT 0.10.0
- Transformers 4.40.1
- Pytorch 2.2.1+cu121
- Datasets 2.16.1
- Tokenizers 0.19.1